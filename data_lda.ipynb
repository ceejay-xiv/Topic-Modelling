{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cHdmXfh6Khgf",
    "outputId": "8a21c600-c242-4a9f-aee2-bbe6d92d92b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting nltk>=3.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting joblib\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tqdm\n",
      "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting click\n",
      "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.6/96.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting regex>=2021.8.3\n",
      "  Downloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m771.9/771.9 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "Successfully installed click-8.1.3 joblib-1.2.0 nltk-3.8.1 regex-2023.3.23 textblob-0.17.1 tqdm-4.65.0\n",
      "Collecting textacy\n",
      "  Downloading textacy-0.12.0-py3-none-any.whl (208 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.4/208.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting catalogue~=2.0\n",
      "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from textacy) (1.2.0)\n",
      "Collecting numpy>=1.17.0\n",
      "  Downloading numpy-1.24.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from textacy) (2.29.0)\n",
      "Collecting cachetools>=4.0.0\n",
      "  Downloading cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyphen>=0.10.0\n",
      "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.19.6 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from textacy) (4.65.0)\n",
      "Collecting jellyfish>=0.8.0\n",
      "  Downloading jellyfish-0.11.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn>=0.19.0\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy>=3.0.0\n",
      "  Downloading spacy-3.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting networkx>=2.0\n",
      "  Downloading networkx-3.1-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=0.17.0\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cytoolz>=0.10.1\n",
      "  Downloading cytoolz-0.12.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting toolz>=0.8.0\n",
      "  Downloading toolz-0.12.0-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from requests>=2.10.0->textacy) (2022.12.7)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (925 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.9/925.9 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: jinja2 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (3.1.2)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
      "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.8/56.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.1-py3-none-any.whl (27 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.8.0,>=0.3.0\n",
      "  Downloading typer-0.7.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: setuptools in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (66.0.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from spacy>=3.0.0->textacy) (23.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.5/493.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy>=3.0.0->textacy) (4.5.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m36m0:00:01\u001b[0mm\n",
      "\u001b[?25hRequirement already satisfied: click<9.0.0,>=7.1.1 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from typer<0.8.0,>=0.3.0->spacy>=3.0.0->textacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/hfsc5/miniconda3/envs/child_grooming/lib/python3.8/site-packages (from jinja2->spacy>=3.0.0->textacy) (2.1.1)\n",
      "Installing collected packages: cymem, wasabi, typer, toolz, threadpoolctl, spacy-loggers, spacy-legacy, smart-open, pyphen, pydantic, numpy, networkx, murmurhash, langcodes, jellyfish, catalogue, cachetools, srsly, scipy, preshed, pathy, cytoolz, blis, scikit-learn, confection, thinc, spacy, textacy\n",
      "Successfully installed blis-0.7.9 cachetools-5.3.0 catalogue-2.0.8 confection-0.0.4 cymem-2.0.7 cytoolz-0.12.1 jellyfish-0.11.2 langcodes-3.3.0 murmurhash-1.0.9 networkx-3.1 numpy-1.24.3 pathy-0.10.1 preshed-3.0.8 pydantic-1.10.7 pyphen-0.14.0 scikit-learn-1.2.2 scipy-1.10.1 smart-open-6.3.0 spacy-3.5.2 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.6 textacy-0.12.0 thinc-8.1.9 threadpoolctl-3.1.0 toolz-0.12.0 typer-0.7.0 wasabi-1.1.1\n",
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.7.2-py3-none-any.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.7.2\n",
      "Collecting autocorrect\n",
      "  Downloading autocorrect-2.6.1.tar.gz (622 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m622.8/622.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
      "  Building wheel for autocorrect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autocorrect: filename=autocorrect-2.6.1-py3-none-any.whl size=622363 sha256=9f85a81085b4b79d693c4218d64dd8ee8750cb968ad91850d62afc1e2219ed89\n",
      "  Stored in directory: /home/hfsc5/.cache/pip/wheels/72/b8/3b/a90246d13090e85394a8a44b78c8abf577c0766f29d6543c75\n",
      "Successfully built autocorrect\n",
      "Installing collected packages: autocorrect\n",
      "Successfully installed autocorrect-2.6.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob\n",
    "!pip install textacy\n",
    "!pip install pyspellchecker\n",
    "!pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zifFmP_hKrZf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import textacy\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VY90fsiqKqpf"
   },
   "outputs": [],
   "source": [
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HMURnrwwTsK5"
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "FqmAz2ulTvlY"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5T_sh1ARQO7Z"
   },
   "outputs": [],
   "source": [
    "with open(\"combined.txt\", \"rb\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cY5VTw63iFjF"
   },
   "outputs": [],
   "source": [
    "data = [item.strip() for item in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "p5XIKY_GUGSz"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'chats': data})\n",
    "df['chats'] = df['chats'].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4VJ9N__935Sa"
   },
   "outputs": [],
   "source": [
    "df['users'] = df['chats'].str.split(n=1).str[0]\n",
    "df['chats'] = df['chats'].str.split(n=1).str[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3wKvR3drkFn2",
    "outputId": "0937de66-dff6-4cd8-a2eb-6094f755e967"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18265/1211096447.py:1: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats'] = df['chats'].str.replace('\\d+', '')#remove numbers\n",
      "/tmp/ipykernel_18265/1211096447.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats']=df['chats'].str.replace(r\"[\\/\\-\\~\\_\\[\\]\\{\\}\\:\\;\\(\\)\\\"\\'\\|\\?\\=\\.\\<\\>\\@\\#\\*\\,]\", '')#remove punctuations\n",
      "/tmp/ipykernel_18265/1211096447.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats'] = df['chats'].str.replace('[^\\w\\s]','')#remove punctuations\n"
     ]
    }
   ],
   "source": [
    "df['chats'] = df['chats'].str.replace('\\d+', '')#remove numbers\n",
    "df['chats']=df['chats'].str.replace(r\"[\\/\\-\\~\\_\\[\\]\\{\\}\\:\\;\\(\\)\\\"\\'\\|\\?\\=\\.\\<\\>\\@\\#\\*\\,]\", '')#remove punctuations\n",
    "df['chats'] = df['chats'].str.replace('[^\\w\\s]','')#remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hCKd6layQCHV",
    "outputId": "c62d5f0c-569e-4b4b-da20-c2b3c2226b6f"
   },
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "exploded_series = df['chats'].str.split(' ').explode()\n",
    "mask = exploded_series.isin(['PM','AM'])\n",
    "df['time'] = exploded_series[mask].groupby(level=0).apply(' '.join)\n",
    "df['chats'] = exploded_series[~mask].groupby(level=0).apply(' '.join)\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "3li2htlQQC_u"
   },
   "outputs": [],
   "source": [
    "df[\"Number of Words\"] = df[\"chats\"].apply(lambda n: len(n.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "badi8zsqQIry",
    "outputId": "76dcf840-9742-4c68-9dfd-33e90797afa3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chats</th>\n",
       "      <th>users</th>\n",
       "      <th>time</th>\n",
       "      <th>Number of Words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46841</th>\n",
       "      <td>the next phone call which lasted approximately...</td>\n",
       "      <td>b'In</td>\n",
       "      <td>PM</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474981</th>\n",
       "      <td>hi love i have look  see if u came back on a...</td>\n",
       "      <td>b'ghost27_73</td>\n",
       "      <td>PM</td>\n",
       "      <td>177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119164</th>\n",
       "      <td>You are beautiful Trust me on this You are t...</td>\n",
       "      <td>b'atlanta.italian</td>\n",
       "      <td>PM</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119163</th>\n",
       "      <td>When you are gone I have time to think You a...</td>\n",
       "      <td>b'atlanta.italian</td>\n",
       "      <td>PM</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120062</th>\n",
       "      <td>Maddie I wanted to drop you a note saying ho...</td>\n",
       "      <td>b'tealitalianeyes</td>\n",
       "      <td>PM</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883321</th>\n",
       "      <td></td>\n",
       "      <td>b'malhotra.mohit</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883322</th>\n",
       "      <td></td>\n",
       "      <td>b'malhotra.mohit</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300947</th>\n",
       "      <td></td>\n",
       "      <td>b'makenna_againa</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300922</th>\n",
       "      <td></td>\n",
       "      <td>b'makenna_againa</td>\n",
       "      <td>PM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>530427</th>\n",
       "      <td></td>\n",
       "      <td>b'herrhollrah</td>\n",
       "      <td>AM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1352907 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    chats              users  \\\n",
       "46841   the next phone call which lasted approximately...               b'In   \n",
       "474981    hi love i have look  see if u came back on a...       b'ghost27_73   \n",
       "119164    You are beautiful Trust me on this You are t...  b'atlanta.italian   \n",
       "119163    When you are gone I have time to think You a...  b'atlanta.italian   \n",
       "120062    Maddie I wanted to drop you a note saying ho...  b'tealitalianeyes   \n",
       "...                                                   ...                ...   \n",
       "883321                                                      b'malhotra.mohit   \n",
       "883322                                                      b'malhotra.mohit   \n",
       "300947                                                      b'makenna_againa   \n",
       "300922                                                      b'makenna_againa   \n",
       "530427                                                         b'herrhollrah   \n",
       "\n",
       "       time  Number of Words  \n",
       "46841    PM              330  \n",
       "474981   PM              177  \n",
       "119164   PM              160  \n",
       "119163   PM              160  \n",
       "120062   PM              157  \n",
       "...     ...              ...  \n",
       "883321   PM                0  \n",
       "883322   PM                0  \n",
       "300947   PM                0  \n",
       "300922   PM                0  \n",
       "530427   AM                0  \n",
       "\n",
       "[1352907 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values(by=['Number of Words'], ascending=[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "gEIdNU-t41lP"
   },
   "outputs": [],
   "source": [
    "df1=df.groupby(['users'])['chats'].apply(','.join).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "wr6VUsnlk-uk"
   },
   "outputs": [],
   "source": [
    "df1[\"Number of Words\"] = df[\"chats\"].apply(lambda n: len(n.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmLzB5T1QiNF",
    "outputId": "2d6cea86-235e-4976-c5cc-6420b26648e3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352907"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = df['chats'].tolist()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QSZ_AGxLldNJ",
    "outputId": "e8a8566a-182b-4742-df32-65c8f7eee0d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1352907"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2 = df1['chats'].tolist()\n",
    "len(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "27UNCNteQ2zh"
   },
   "outputs": [],
   "source": [
    "corpus = list(dict.fromkeys(corpus))\n",
    "#Remove words with less than 2 characters\n",
    "corpus = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in corpus]\n",
    "corpus = list(dict.fromkeys(corpus))\n",
    "corpus = list(filter(None, corpus))\n",
    "corpus = [c.strip(' ') for c in corpus]\n",
    "corpus = [c.lower() for c in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "6Yc45djipbKU"
   },
   "outputs": [],
   "source": [
    "def check_long(lst):\n",
    "    return [item for item in lst if len(item) > 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuMv2YSBjH2U",
    "outputId": "f2ed9ec3-4c82-47b2-9a61-741ad664f148"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/hfsc5/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "NoG-NW-ujH2U"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgQzqgL92-e7",
    "outputId": "a69a846d-2389-4aeb-f2ba-91d1fcf66c8a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hfsc5/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "5e704ab5"
   },
   "outputs": [],
   "source": [
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "2ib_R5NdMLcA"
   },
   "outputs": [],
   "source": [
    "corpus_2 = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in corpus]\n",
    "corpus_2 = list(dict.fromkeys(corpus_2))\n",
    "corpus_2 = list(filter(None, corpus_2))\n",
    "corpus_2 = [c.strip(' ') for c in corpus_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "AZ6k6oBgBGrP"
   },
   "outputs": [],
   "source": [
    "in_words =[]\n",
    "for line in corpus_2:\n",
    "    for word in line.split():\n",
    "        if (word.endswith('in')) and (word not in in_words):\n",
    "            in_words.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 336
    },
    "id": "M0GFBSNFBd_v",
    "outputId": "7de56207-d612-4249-ede2-c54f0267f0d6"
   },
   "outputs": [],
   "source": [
    "word_list = brown.words()\n",
    "word_list = [wl.lower() for wl in word_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "RQBcXVcXnCGO"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "wnl = WordNetLemmatizer()\n",
    "word_lem=[]\n",
    "to_change_in=[]\n",
    "\n",
    "for w in in_words:\n",
    "    if (wnl.lemmatize(''.join([w,'g']), pos='v') in word_list) and (w!='thin'):\n",
    "        to_change_in.append(w)\n",
    "        word_lem.append(wnl.lemmatize(''.join([w,'g']), pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZIqMkJEp0jU9"
   },
   "outputs": [],
   "source": [
    "def get_tuple(list1, list2):\n",
    "    merged_list = [(list1[i], list2[i]) for i in range(0, len(list1))]\n",
    "    return merged_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "KoZhf1ndlIs3"
   },
   "outputs": [],
   "source": [
    "tuple_pair_in = get_tuple(to_change_in, word_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "HKkjmvcklIgb"
   },
   "outputs": [],
   "source": [
    "# tuple_pair_in[:-20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "0W0XsaPm87qb"
   },
   "outputs": [],
   "source": [
    "temp_str = str(corpus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "MKERHjOl6Ini"
   },
   "outputs": [],
   "source": [
    "for old, new in tuple_pair_in:\n",
    "    temp_str = temp_str.replace(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "JgocSLRnHDUu"
   },
   "outputs": [],
   "source": [
    "temp_str=temp_str.replace('gg','g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "aZcyjCXzNOJ3"
   },
   "outputs": [],
   "source": [
    "fix_words = ['suked', 'fuked', 'suk', 'fuk', 'suking', 'fuking']\n",
    "fixed_words = ['sucked', 'fucked','suk', 'fuck', 'sucking', 'fucking']\n",
    "fix_tuple = get_tuple(fix_words, fixed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "2ofUhzc4oLL4"
   },
   "outputs": [],
   "source": [
    "fixed_words = ['sucked', 'fucked','suk', 'fuck', 'sucking', 'fucking', 'porn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "r_MWRWTgN6mr"
   },
   "outputs": [],
   "source": [
    "for old, new in fix_tuple:\n",
    "    temp_str = temp_str.replace(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "zW9wHhiL-4WD"
   },
   "outputs": [],
   "source": [
    "chats_1 = temp_str.split(',')\n",
    "chats_1 = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in chats_1]\n",
    "chats_1 = list(dict.fromkeys(chats_1))\n",
    "chats_1 = list(filter(None, chats_1))\n",
    "chats_1 = [c.strip(' ') for c in chats_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Zusmcoinywg6"
   },
   "outputs": [],
   "source": [
    "import textacy \n",
    "\n",
    "patterns = [{'POS':'X'}]\n",
    "\n",
    "other_words2 = []\n",
    "\n",
    "for chat in chats_1:\n",
    "    doc= nlp(chat)\n",
    "    other_text = textacy.extract.token_matches(doc, patterns = patterns)\n",
    "    for o in other_text:\n",
    "        rmv_txt = str(o)\n",
    "        if (rmv_txt not in other_words2):\n",
    "            other_words2.append(rmv_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "G4kTeJJK985k"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18265/1749365649.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats'] = df['chats'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'chats': chats_1})\n",
    "df['chats'] = df['chats'].astype(\"string\")\n",
    "df['chats'] = df['chats'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "lRfuulgIyU8B"
   },
   "outputs": [],
   "source": [
    "data = df.chats.tolist()\n",
    "data = list(filter(None, data))\n",
    "data = list(dict.fromkeys(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "oH5nqm110SD8"
   },
   "outputs": [],
   "source": [
    "data_str = str(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ebtCvF3wp2VB"
   },
   "outputs": [],
   "source": [
    "with open('short_words.txt', 'r') as f:\n",
    "    short_words = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "WKUQxaHRqu74"
   },
   "outputs": [],
   "source": [
    "short_words = [re.sub('\\s+', ' ', s) for s in short_words]\n",
    "short_words = [re.sub('\\d+', ' ', s) for s in short_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "xiD6CKsM4qna"
   },
   "outputs": [],
   "source": [
    "short_words2 = []\n",
    "for sw in short_words:\n",
    "    short_words2.append(' '+sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "cNfw3LZDqQ0B"
   },
   "outputs": [],
   "source": [
    "for sw in short_words2:\n",
    "    data_str=data_str.replace(sw, ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "DQXMfMRatf5d"
   },
   "outputs": [],
   "source": [
    "data_2 = data_str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "FdHeVh4Gt9rC"
   },
   "outputs": [],
   "source": [
    "lol_ptrn = 'lol\\w+'\n",
    "lol_words = re.findall(lol_ptrn, data_str)\n",
    "lol_words = list(dict.fromkeys(lol_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "klU7VcMNvLJQ"
   },
   "outputs": [],
   "source": [
    "lol_ptrn = 'lol\\w+'\n",
    "lol_words = re.findall(lol_ptrn, data_str)\n",
    "lol_words = list(dict.fromkeys(lol_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "y6Gj1nLjuKQO"
   },
   "outputs": [],
   "source": [
    "lol_words2=[]\n",
    "for line in data_2:\n",
    "    for word in line.split():\n",
    "        if ('lol' in word) and (word not in lol_words2):\n",
    "            lol_words2.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "s7PaDHOHuGk9"
   },
   "outputs": [],
   "source": [
    "for i in lol_words:\n",
    "    data_str = data_str.replace(i,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "BelQofKO4X7T"
   },
   "outputs": [],
   "source": [
    "for i in lol_words2:\n",
    "    data_str = data_str.replace(i,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "sG3_54x2HfyO"
   },
   "outputs": [],
   "source": [
    "ok_ptrn = 'ok\\w+'\n",
    "ok_words = re.findall(ok_ptrn, data_str)\n",
    "ok_words = list(dict.fromkeys(ok_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "UX_5oWXhHrKr"
   },
   "outputs": [],
   "source": [
    "for i in ok_words:\n",
    "    data_str = data_str.replace(i,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ymlVEWKe1MXT"
   },
   "outputs": [],
   "source": [
    "ing_ptrn = 'ing\\w+'\n",
    "ing_words = re.findall(ing_ptrn, data_str)\n",
    "ing_words = list(dict.fromkeys(ing_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "mjINKoG558b1"
   },
   "outputs": [],
   "source": [
    "ing_words2=[]\n",
    "for i in ing_words:\n",
    "    ing_words2.append(' '+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Hba3r7Hq1MXT"
   },
   "outputs": [],
   "source": [
    "# cocked_ptrn = 'cocked\\w+'\n",
    "# cocked_words = re.findall(cocked_ptrn, data_str)\n",
    "# cocked_words = list(dict.fromkeys(cocked_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "L53vFKpq1evW"
   },
   "outputs": [],
   "source": [
    "for i in ing_words2:\n",
    "    data_str = data_str.replace(i,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "B3gzAWz81MXU"
   },
   "outputs": [],
   "source": [
    "# cocked_words2=[]\n",
    "# for c in cocked_words:\n",
    "#     cocked_words2.append(' '+c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5gl-wTwJ1qtF"
   },
   "outputs": [],
   "source": [
    "# def add_ws(string, length):\n",
    "#     return ' '.join(string[i:i+length] for i in range(0,len(string),length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "8Eyieda34ECq"
   },
   "outputs": [],
   "source": [
    "# cocked_words2=[]\n",
    "\n",
    "# for c in cocked_words:\n",
    "#     cocked_words2.append(add_ws(c, 6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "yLiJLOIk71jD"
   },
   "outputs": [],
   "source": [
    "# tuple_cocked = get_tuple(cocked_words, cocked_words2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "bngXBAf-7EJc"
   },
   "outputs": [],
   "source": [
    "# for old, new in tuple_cocked:\n",
    "#     corpus_str = corpus_str.replace(old,new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "_2Om1gELXvRn"
   },
   "outputs": [],
   "source": [
    "chats = data_str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "hKVZr0IyzOmI"
   },
   "outputs": [],
   "source": [
    "#Remove words with less than 2 characters\n",
    "chats = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in chats]\n",
    "chats = list(dict.fromkeys(chats))\n",
    "chats = list(filter(None, chats))\n",
    "chats = [c.strip(' ') for c in chats]\n",
    "chats = [c.lower() for c in chats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "H4BmL1hQtwNI"
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'chats': chats})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "KLUiW18iokB0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18265/3449276781.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats'] = df['chats'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df['chats'] = df['chats'].astype(\"string\")\n",
    "df['chats'] = df['chats'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "rTCVH4ygwwUL"
   },
   "outputs": [],
   "source": [
    "df['chats']=df['chats'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "5nxCOR2wxuPm"
   },
   "outputs": [],
   "source": [
    "df['chats'] = df['chats'].apply(lambda x: re.split('http\\/\\/.*', str(x))[0])\n",
    "df['chats'] = df['chats'].apply(lambda x: re.split('https\\/\\/.*', str(x))[0])\n",
    "df['chats'] = df['chats'].apply(lambda x: re.split('www\\/\\/.*', str(x))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "eVq0Fcl3_aL3"
   },
   "outputs": [],
   "source": [
    "def remove_whitespaces(text):\n",
    "    rmv_ws = re.sub(' +', ' ' ,text)\n",
    "    return rmv_ws\n",
    "\n",
    "def remove_hl(text):\n",
    "    final = re.sub('<[a][^>]*>(.+?)</[a]>', 'Link.', text)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "WctlUcnrY2Re"
   },
   "outputs": [],
   "source": [
    "def cleaning(x, chat):\n",
    "    df_x = x.copy()\n",
    "    df_x[chat]= df_x['chats']\n",
    "    df_x[chat]= df_x[chat].apply(remove_whitespaces)\n",
    "    df_x[chat]= df_x[chat].apply(remove_hl)\n",
    "    df_x[chat] = df_x[chat].dropna()\n",
    "    return df_x\n",
    "\n",
    "df_x = cleaning(df, 'chats')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "aZ4hcOCie5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "740322\n"
     ]
    }
   ],
   "source": [
    "corpus = df_x.chats.tolist()\n",
    "corpus = list(dict.fromkeys(corpus))\n",
    "corpus = list(filter(None, corpus))\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "BF9ZyUhdoPMG"
   },
   "outputs": [],
   "source": [
    "to_change=[]\n",
    "changed = []\n",
    "for line in corpus:\n",
    "    for word in line.split():\n",
    "        if len(word)>=13:\n",
    "            to_change.append(word)\n",
    "            changed.append(re.sub(r'(.)\\1+', r'\\1', word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "cFiRnSexHbOD"
   },
   "outputs": [],
   "source": [
    "tuple_correct = get_tuple(to_change, changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "KTeOOl8CHbOJ"
   },
   "outputs": [],
   "source": [
    "corpus_str = str(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "nQzH2uhvHbOJ"
   },
   "outputs": [],
   "source": [
    "for old, new in tuple_correct:\n",
    "    corpus_str = corpus_str.replace(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "4LqbDHmPHbOJ"
   },
   "outputs": [],
   "source": [
    "corpus_2 = corpus_str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "7eTNyREnLMWd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['hi'\",\n",
       " \" 'pftloser'\",\n",
       " \" 'sup'\",\n",
       " \" 'location'\",\n",
       " \" 'hello'\",\n",
       " \" 'srry was sleep'\",\n",
       " \" 'farmington hills'\",\n",
       " \" 'canton'\",\n",
       " \" 'do you have picture'\",\n",
       " \" 'yea trade'\",\n",
       " \" 'mine is in my profile'\",\n",
       " \" 'profis not work me can send'\",\n",
       " \" 'please send me yours will reply'\",\n",
       " \" 'kk sent'\",\n",
       " \" 'handsome boy'\",\n",
       " \" 'am years old'\",\n",
       " \" 'im'\",\n",
       " \" 'send'\",\n",
       " \" 'wow'\",\n",
       " \" 'dont luk that old'\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_2[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "RMbx9PPPK68L"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/hfsc5/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "VxJ3t0VKHyPI"
   },
   "outputs": [],
   "source": [
    "#Remove words with less than 2 characters\n",
    "corpus_2 = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in corpus_2]\n",
    "corpus_2 = list(dict.fromkeys(corpus_2))\n",
    "corpus_2 = list(filter(None, corpus_2))\n",
    "corpus_2 = [c.strip(' ') for c in corpus_2]\n",
    "corpus_2 = [c.lower() for c in corpus_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "8ky9S-wBzCxC"
   },
   "outputs": [],
   "source": [
    "unique = []\n",
    "for line in corpus_2:\n",
    "    for word in line.split(' '):\n",
    "        if word not in unique:\n",
    "            unique.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "b1e1ba2f"
   },
   "outputs": [],
   "source": [
    "correct_words = []\n",
    "incorrect_words = []\n",
    "def spell_checker(word):\n",
    "    word = Word(word)\n",
    "    final = word.spellcheck()\n",
    "    if (wnl.lemmatize(final[0][0], pos='v')):\n",
    "        if (final[0][1] > 0.56) and (final[0][1] < 0.9) and (word not in fixed_words) and (word not in word_list):\n",
    "            incorrect_words.append(word)\n",
    "            correct_words.append(final[0][0])\n",
    "            # print(f'|\"{word}\": \"{final[0][0]}\"  | Confidence-> {final[0][1]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "3061e153"
   },
   "outputs": [],
   "source": [
    "for word in unique:\n",
    "    spell_checker(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "gpI-nZuakgUu"
   },
   "outputs": [],
   "source": [
    "incorrect_w2 = []\n",
    "for word in incorrect_words:\n",
    "    incorrect_w2.append(' '+word+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "v3dMcmyqBr_v"
   },
   "outputs": [],
   "source": [
    "correct_w2 = []\n",
    "for word in correct_words:\n",
    "    correct_w2.append(' '+word+' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "p2D4Ny_zoSVm"
   },
   "outputs": [],
   "source": [
    "incorrect_w3=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "8zXYh0Hkkpmc"
   },
   "outputs": [],
   "source": [
    "incorrect_w3 = [re.sub(r'[^\\w\\s]','', d) for d in incorrect_w2]\n",
    "correct_w2 = [re.sub(r'[^\\w\\s]','', d) for d in correct_w2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "qQ6oa4_MqAAl"
   },
   "outputs": [],
   "source": [
    "tuple_correct_1 = get_tuple(incorrect_w3, correct_w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "VhWXJZ0w3k6p"
   },
   "outputs": [],
   "source": [
    "corpus_str = str(corpus_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "6CYIFIuJGy3Y"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16141"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tuple_correct_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "bLP1m8DS3MSy"
   },
   "outputs": [],
   "source": [
    "for old, new in tuple_correct_1:\n",
    "    corpus_str = corpus_str.replace(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "id": "GKtlOPRNoVzO"
   },
   "outputs": [],
   "source": [
    "temp = corpus_str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "Hy-sNC1AH5uC"
   },
   "outputs": [],
   "source": [
    "temp = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in temp]\n",
    "temp = list(dict.fromkeys(temp))\n",
    "temp = list(filter(None, temp))\n",
    "temp = [c.strip(' ') for c in temp]\n",
    "temp = [c.lower() for c in temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "SnCsFWtPIVCp"
   },
   "outputs": [],
   "source": [
    "df=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "id": "fXge3L_vHSSo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18265/2229609577.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats'] = df['chats'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({'chats': temp})\n",
    "df['chats'] = df['chats'].astype(\"string\")\n",
    "df['chats'] = df['chats'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "5V3T_OtUIfX3"
   },
   "outputs": [],
   "source": [
    "temp_2 = df.chats.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "gdoUClo1I4IO"
   },
   "outputs": [],
   "source": [
    "temp_2 = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in temp_2]\n",
    "temp_2 = list(dict.fromkeys(temp_2))\n",
    "temp_2 = list(filter(None, temp_2))\n",
    "temp_2 = [c.strip(' ') for c in temp_2]\n",
    "temp_2 = [c.lower() for c in temp_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "id": "lHMv-7YjY6R9"
   },
   "outputs": [],
   "source": [
    "# import textacy \n",
    "\n",
    "# patterns = [{'POS':'X'}]\n",
    "\n",
    "# other_words = []\n",
    "\n",
    "# for chat in temp:\n",
    "#     doc= nlp(chat)\n",
    "#     other_text = textacy.extract.token_matches(doc, patterns = patterns)\n",
    "#     for o in other_text:\n",
    "#         rmv_txt = str(o)\n",
    "#         if (rmv_txt not in other_words):\n",
    "#             other_words.append(rmv_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "-7dArwuelJox"
   },
   "outputs": [],
   "source": [
    "keep_words=['heaven','transformer','risingg','snowy','drinkingg','lastnite','feb','rue','rubbingg','talkingg','makingg','versus','anythingg','christmas','english','girlish','afterschol','condemn','jokingg','anyproblem','africa','penis','caucasian','thighs','jewel','spagheti']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "id": "iowMUKmrk2-6"
   },
   "outputs": [],
   "source": [
    "# other_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "mPjPfMHJfBPl"
   },
   "outputs": [],
   "source": [
    "# other_w = [word for word in other_words if word not in keep_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "QTtbaDtboFMj"
   },
   "outputs": [],
   "source": [
    "chat_str = str(temp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "VnTqB8j5m5Sx"
   },
   "outputs": [],
   "source": [
    "str_2=re.findall(\"[a-zA-Z,.]+\",chat_str)\n",
    "chat_docx=(\" \".join(str_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "id": "DAYn42IooMd5"
   },
   "outputs": [],
   "source": [
    "# for o in other_w:\n",
    "#     chat_docx2=chat_docx.replace(o, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "U7BDUznqrs6I"
   },
   "outputs": [],
   "source": [
    "# chat_docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "id": "M1M-K1pMqlIw"
   },
   "outputs": [],
   "source": [
    "chat_list = chat_docx.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "FTzapBaIvgSW"
   },
   "outputs": [],
   "source": [
    "pp = []\n",
    "for line in chat_list:\n",
    "    for word in line.split():\n",
    "        if (len(word)>=12) and (word not in pp) and (word not in word_list):\n",
    "            pp.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "eb4klfs2weNu"
   },
   "outputs": [],
   "source": [
    "xx = []\n",
    "for x in pp:\n",
    "    xx.append((re.sub(r'(.)\\1+', r'\\1', x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "id": "hF3fRhw2Jole"
   },
   "outputs": [],
   "source": [
    "tuple_xp = get_tuple(pp, xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "id": "pERjpnJeLlh0"
   },
   "outputs": [],
   "source": [
    "# tuple_xp[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "1lioqclhv6s4"
   },
   "outputs": [],
   "source": [
    "for old, new in tuple_xp:\n",
    "    chat_docx = chat_docx.replace(old, new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "id": "HkAhibtDNbj-"
   },
   "outputs": [],
   "source": [
    "chat_list = chat_docx.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "id": "46e_v-99yNXm"
   },
   "outputs": [],
   "source": [
    "chat_docx=chat_docx.replace(' ill ','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "k-Dk38ocxFD-"
   },
   "outputs": [],
   "source": [
    "chat_list = chat_docx.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "id": "8sNzXaCjxHSV"
   },
   "outputs": [],
   "source": [
    "# chat_list[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "id": "QnduBpdQZBZA"
   },
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True, min_len=1, max_len=20))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sent_to_words(chat_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "_kFSJ6NAcYWv"
   },
   "outputs": [],
   "source": [
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "id": "nhpgdnQzdGQo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hfsc5/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "id": "uJTuZ9syPufS"
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "3LKjMdN1dOvs"
   },
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Form trigrams\n",
    "data_words_trigrams = make_trigrams(data_words_nostops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "id": "YRnC_23EPztm"
   },
   "outputs": [],
   "source": [
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_trigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "id": "IEKcvwuyYnry"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_18265/3260702434.py:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['chats'] = df['chats'].str.replace('[^\\w\\s]','')\n"
     ]
    }
   ],
   "source": [
    "df=[]\n",
    "df = pd.DataFrame({'chats': data_lemmatized})\n",
    "df['chats'] = df['chats'].astype(\"string\")\n",
    "df['chats'] = df['chats'].str.replace('[^\\w\\s]','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "id": "VYezTh7JY7Lr"
   },
   "outputs": [],
   "source": [
    "final_df = df.chats.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "id": "29p7mLsPxR5m"
   },
   "outputs": [],
   "source": [
    "final_df = [re.sub(r'\\b\\w{0,1}\\b', '', d) for d in final_df]\n",
    "final_df = list(dict.fromkeys(final_df))\n",
    "final_df = list(filter(None, final_df))\n",
    "final_df = [c.strip(' ') for c in final_df]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "id": "Egpf_r6q0Ilv"
   },
   "outputs": [],
   "source": [
    "# corpus[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "id": "OsuCS1rfW3wI"
   },
   "outputs": [],
   "source": [
    "# # Build the bigram and trigram models\n",
    "# bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "# trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "\n",
    "# # Faster way to get a sentence clubbed as a trigram/bigram\n",
    "# bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "# trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "id": "YjKCQ4GLW3wI"
   },
   "outputs": [],
   "source": [
    "# def remove_stopwords(texts):\n",
    "#     return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "# def make_bigrams(texts):\n",
    "#     return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "# def make_trigrams(texts):\n",
    "#     return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "# def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "#     \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "#     texts_out = []\n",
    "#     for sent in texts:\n",
    "#         doc = nlp(\" \".join(sent)) \n",
    "#         texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "#     return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "id": "CKL5WoXyW3wI"
   },
   "outputs": [],
   "source": [
    "# # Do lemmatization keeping only noun, adj, vb, adv\n",
    "# data_lemmatized = lemmatization(data_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "id": "cphCTzNv75xK"
   },
   "outputs": [],
   "source": [
    "with open(\"final_corpus.txt\", \"w\") as f:\n",
    "    for text in final_df:\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
